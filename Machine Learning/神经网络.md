# CNN 卷积神经网络
卷积神经网络(CNN)常用于图像识别，先简单介绍一下CNN的发展历史
### 1. LeNet-5 (1998)
作用于32x32的灰度图像的7层神经网络，也是神经网络最早的架构，当时这个网络被计算资源所限制，因为1998年的电脑并不那么好用。
<img src="https://github.com/mingming741/RenneNotes/blob/master/Resource/Image/LeNet1998.png"/>

### 2. AlexNet (2012)
比LeNet稍微深一点点，包括11x11, 5x5,3x3的不同size的卷积, max pooling, dropout机制，在卷积层和全连接层后面有ReLU激励层, 使用SGD，并且有momentum，有data augmentation（即增加训练集的数量，比如对图片进行旋转，切割等，产生不同的新图片，而不是去寻找新图片）。算是首次比较完善的卷积神经网络结构
<img src="https://github.com/mingming741/RenneNotes/blob/master/Resource/Image/AlexNet2012.png"/>

### 3. ZFNet(2013)
和AlexNet的结构差不多，主要是通过调整超参数实现的更好的结果
<img src="https://github.com/mingming741/RenneNotes/blob/master/Resource/Image/ZFNet2013.png"/>

### 4. GoogleNet/Inception(2014)
首次接近人眼识别的错误率的神经网络，配合了inception module，使用了batch normalization, image distortions and RMSprop，中间有一些减少参数的方法，使得22层的神经网络的参数减少了15倍。

<img src="https://github.com/mingming741/RenneNotes/blob/master/Resource/Image/Inception2014.png"/>

### 5. VGGNet (2014)
16个卷积层，后面接pooling层，并且分布的非常均匀，并且只使用了3x3的卷积。VGG中有batch normalization的机制。VGG是现在提取图像特征最好用的网络，但是参数的数量非常多，难以训练。
<img src="https://github.com/mingming741/RenneNotes/blob/master/Resource/Image/VGG2014.png"/>

### 6. ResNet(2015)
Residual Neural Network(ResNet)，Kaiming大佬提出来的思想，中间有skip connections的方法，即每一层之间并不是之间相连下一层，而是隔层的相连方式，152层的ResNet可以做到比VGG的复杂度更低，并且达到更好的效果。中间还使用了features heavy batch normalization.
<img src="https://github.com/mingming741/RenneNotes/blob/master/Resource/Image/ResNet2015.png"/>



# MLP (Multi-layer Perceptron)多层感知机
多层感知器即多个单层感知器链接而成，下图表示单层感知机(PLA: Perceptron Learning Algorithm)的的结构，其实就是一个线性分类器的算法。更准确的说，PLA是一个线性的二分类器，但不能对非线性的数据并不能进行有效的分类，所以在这里引用多层的结构，理论上，多层的网络可以模拟复杂的函数。
<br/><img src = "https://github.com/mingming741/RenneNotes/blob/master/Resource/Image/多层感知器_单层.png" width = 375/><br/>
而多层的分类器本质上是全链接层和激活层的嵌套链接，最后可以达到输出多个结果的效果
<br/><img src = "https://github.com/mingming741/RenneNotes/blob/master/Resource/Image/多层感知器_多层.png" width = 375/><br/>

