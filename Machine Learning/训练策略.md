### Batch normalization
通常是在Conv之前，会对batch做batch normalization，（在torch里面是使用一个batch normalization的layer，对每个training mini-batch做一次）。batch normalization的好处是可以加速训练过程。可以理解为将分散的数据标准化。使用batch normalization可以使得数据集对initialization变得不那么敏感，并且可以允许我们使用更高的learning rate。有些情况下，batch normalization可以代替dropout的效果，消除overfitting的效果。<br/>
参考Paper：<a herf = 'https://github.com/mingming741/RenneNotes/blob/master/Paper/Image%20NN/Paper_Batch_Nonalization.pdf'>Batch Normalization</a>
